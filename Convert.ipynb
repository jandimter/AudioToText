{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversión audio-texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El documento contempla dos alternativas:\n",
    "1. Google Speech to text\n",
    "2. Vosk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Speech-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se contempla el uso de el SDK de Google y por tanto de sus credenciales.\n",
    "#pip install google-cloud-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_with_diarization_and_punctuation_async(gcs_uri, output_txt_path):\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"es-ES\",\n",
    "        enable_speaker_diarization=True,\n",
    "        #model=\"video\",  # Utilizar el modelo 'video'\n",
    "        diarization_speaker_count=2,\n",
    "        enable_automatic_punctuation=True\n",
    "    )\n",
    "\n",
    "    operation = client.long_running_recognize(config=config, audio=audio)\n",
    "    response = operation.result(timeout=600)  # Aumentar el tiempo de espera si es necesario\n",
    "    \n",
    "    with open(output_txt_path, \"w\") as txt_file:\n",
    "        for i, result in enumerate(response.results):\n",
    "            alternative = result.alternatives[0]\n",
    "            segment_text = f\"Segmento {i+1}:\\n\"\n",
    "            transcript_text = f\"Transcripción: {alternative.transcript}\\n\"\n",
    "            \n",
    "            # Manejo de errores para el campo speaker_tag\n",
    "            try:\n",
    "                speaker_text = f\"Interlocutor: {alternative.speaker_tag}\\n\"\n",
    "            except AttributeError:\n",
    "                speaker_text = \"Interlocutor: Desconocido\\n\"\n",
    "            \n",
    "            print(segment_text)\n",
    "            print(transcript_text)\n",
    "            print(speaker_text)\n",
    "            print()\n",
    "\n",
    "            txt_file.write(segment_text)\n",
    "            txt_file.write(transcript_text)\n",
    "            txt_file.write(speaker_text)\n",
    "            txt_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmento 1:\n",
      "\n",
      "Transcripción: No, la conocí sería un poco respetuoso hablar sobre una persona que si existió o no, entonces es una inspirado en ese caso y solamente es eso lo que yo tomo en términos de lo que se ocurrió un término concreto, pero no es mía y tu familia por supuesto no tiene nada que ver con no tengo idea como la familia, pero no lo ejerzo obviamente, no? Entonces sí que fue un pie un pie forzado, puede ser como una inspiración o un claro exactamente un punto inicio.\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 2:\n",
      "\n",
      "Transcripción:  Los religiosos sí, una cosa más bien. Te llamo mucho la atención es todo justo.\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 3:\n",
      "\n",
      "Transcripción:  Perdona, disculpa ya, yo creo que es súper importante lo que tú comentas, porque se activa mente, hay una cuestión entorno, lo espiritual y religioso que todos con los que todos nos podemos identificar de una buena manera. Ya sea si tú tienes una YouTube forma parte de una religión convencional como el gato si formas parte de una congregación y también hay un personaje en la novela, qué es Marcia y es una compañera de la amiga de ella del gimnasio, ella es la secretaria y gimnasio ya entonces ya tira un poco ese discurso, haz evangelización en fin de semana. Entonces tiro un poco ese discurso, pero mientras que Sara es la protagonista y su padre son\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 4:\n",
      "\n",
      "Transcripción:  Tenéis te digo ateo por decirte ellos no hablan de eso y tampoco los panfletos, ni estanco está gente que te tire, me da también su credo, no, pero en sus vidas, ellos no nos quieren tener que ver con nada religioso nada de eso, entonces es una cosa bien típica en nuestra sociedad, pero aparte está la dimensión más espiritual que es donde se desenvuelven todos ellos porque en la mitad que una construcción idiosincrásica muy muy potente en nuestra cultura culturas como en España pero sigue en Chile tiene un sello, particularmente idiosincrásico chileno ahí. Tú ves que la gente acude a esta niñita por necesidades espirituales no sea la espiritualidad se ve como una búsqueda una búsqueda de sintonía de\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 5:\n",
      "\n",
      "Transcripción:  Sincronizar mientras que la religión la religión tú tienes las respuestas o tú tienes si tú eres forma parte una determinada religión, sabes que hacer no que pasó seguir, por ejemplo en un río está todo muy estipulado es la religión. Tú tienes las respuestas en la religión en la espiritualidad, tú buscas respuestas entonces más ambiguo en ayunas es mucho más rica culturalmente y en el caso de las animitas se cruzan de cursos supersticiosos en es como un bombo entre pagano esotérico gente proyecta todo tipo de que van mucho más allá de la de las normas y las reglas religiosas no construye. Tu propio construye tu propia espiritualidad creo que lo único lo que pasa con eso yo creo en ti, pero tú ayúdame lo que tengo tacto esté en el caso de la mitad si tú ves los mensajes que la gente de\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 6:\n",
      "\n",
      "Transcripción:  Porque muchos van y dejan texto años, pues no sé, tú escribe o quien te deja cartas tarjetas con hacia la niña hermosa y mucho mucha gente si tú lees testimonios de gente muchos dicen. Oye, yo te estoy regalando este peluche que es muy importante para mí me lo regaló mi mamá cuando yo tenía 8 años en una etapa muy importante en mi vida y yo te estoy ahora este día entonces está también la noción de inversión, no estoy dando cualquier cosa, si no hay que es como golleto, vale? El peso a lo que te estoy preguntando no cualquier bagatela, es muy importante para mí y por lo tanto espero que esté a la altura tu milagro, que lo que yo estoy esperando, no, que puede ser una situación. Te saludo mi negocio. Está quebrando. Necesito ayuda para que te lleva paz dar un examen de grado lo que sea, entonces está esa mezcla entre espiritualidad y paganas.\n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n",
      "Segmento 7:\n",
      "\n",
      "Transcripción: \n",
      "\n",
      "Interlocutor: Desconocido\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uso de la función\n",
    "gcs_uri = \"gs://\"\n",
    "transcribe_audio_with_diarization_and_punctuation_async(gcs_uri, \"Result.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/jan/Downloads/vosk-model-es-0.42/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /Users/jan/Downloads/vosk-model-es-0.42/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /Users/jan/Downloads/vosk-model-es-0.42/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/jan/Downloads/vosk-model-es-0.42/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /Users/jan/Downloads/vosk-model-es-0.42/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /Users/jan/Downloads/vosk-model-es-0.42/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from /Users/jan/Downloads/vosk-model-es-0.42/rnnlm/final.raw\n"
     ]
    }
   ],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "# Cargar el modelo\n",
    "model = Model(\"/Users/jan/Downloads/vosk-model-es-0.42\")\n",
    "\n",
    "# Configurar el reconocedor\n",
    "rec = KaldiRecognizer(model, 16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/jan/Downloads/vosk-model-es-0.42/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /Users/jan/Downloads/vosk-model-es-0.42/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /Users/jan/Downloads/vosk-model-es-0.42/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/jan/Downloads/vosk-model-es-0.42/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /Users/jan/Downloads/vosk-model-es-0.42/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /Users/jan/Downloads/vosk-model-es-0.42/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from /Users/jan/Downloads/vosk-model-es-0.42/rnnlm/final.raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción final almacenada en el archivo de texto.\n"
     ]
    }
   ],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import soundfile as sf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo\n",
    "model = Model(\"/Users/jan/Downloads/vosk-model-es-0.42\")\n",
    "\n",
    "# Configurar el reconocedor\n",
    "rec = KaldiRecognizer(model, 16000)\n",
    "\n",
    "transcription_text = \"\"\n",
    "\n",
    "# Leer el archivo de audio\n",
    "with sf.SoundFile(\"/Users/jan/Desktop/audio_normalized.wav\") as f:\n",
    "    samplerate = f.samplerate\n",
    "    rec = KaldiRecognizer(model, samplerate)\n",
    "    while True:\n",
    "        data = f.buffer_read(2000, dtype='int16')\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        np_data = np.frombuffer(data, dtype=np.int16)\n",
    "        if rec.AcceptWaveform(np_data.tobytes()):\n",
    "            result_str = rec.Result()\n",
    "            result_dict = json.loads(result_str)\n",
    "            transcription = result_dict.get('text', '')\n",
    "            transcription_text += transcription + \" \"\n",
    "\n",
    "# Obtener el resultado final\n",
    "result_str = rec.FinalResult()\n",
    "result_dict = json.loads(result_str)\n",
    "final_transcription = result_dict.get('text', '')\n",
    "transcription_text += final_transcription\n",
    "\n",
    "# Guardar en un archivo de texto\n",
    "with open(\"/Users/jan/Desktop/vosk.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(transcription_text)\n",
    "\n",
    "print(\"Transcripción final almacenada en el archivo de texto.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflexiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La API de Google tiene el importante plus de ser capaz de detectar interlocutores. Cuenta con compatibilidad para castellano de España y castellano de Chile, sin embargo, el primero posee muchas más herramientas disponibles, al mismo tiempo que detecta de mejor manera las palabras del audio. Por otra parte, Vosk mostró un mejor desempeño en la detección del audio, sin embargo, el resultado no presenta una estructura entre interlocutores ni distinguiendo párrafos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión de M4A a WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def convert_m4a_to_wav(m4a_path, wav_path):\n",
    "    audio = AudioSegment.from_file(m4a_path, format=\"m4a\")\n",
    "    audio = audio.set_channels(1).set_frame_rate(16000)  # Configurar para un solo canal y 16000 Hz\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# Uso de la función\n",
    "convert_m4a_to_wav(\"/Users/jan/Desktop/Recortado.m4a\", \"/Users/jan/Desktop/Recortado.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización del volumen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='/Users/jan/Desktop/audio_normalized.wav'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Cargar el archivo de audio\n",
    "audio = AudioSegment.from_file(\"Recortado.wav\")\n",
    "\n",
    "target_dBFS = -20\n",
    "\n",
    "# Normalizar al volumen deseado (dB)\n",
    "normalized_audio = audio.apply_gain(-audio.dBFS + target_dBFS)\n",
    "\n",
    "# Guardar el archivo de audio preprocesado\n",
    "normalized_audio.export(\"audio_normalized.wav\", format=\"wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jan3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
